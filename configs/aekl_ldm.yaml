general:
  exp_name: Dawn_b2
  network: AEKL_LDM
  training_epochs: 250
  eval_freq: 5

ldm:
  base_lr: 0.000025
  # params:
  #   spatial_dims: 3
  #   in_channels: 3
  #   out_channels: 3
  #   num_res_blocks: 2
  #   num_channels: [256, 512, 768]
  #   attention_levels: [False, True, True]
  #   with_conditioning: True
  #   cross_attention_dim: 1024
  #   num_head_channels: [0, 512, 768]
  dmunet: # BASE
    dim: 16                                                  # Base dimension of the unet
    text_embed_dim: 512                                       # dimension of the resnet induced embeddings
    num_resnet_blocks: 2                                      # num of resnetblocks to use on each stage
    dim_mults: [1,2,4]                                        # multiplicators of each stage    
    max_text_len: 512                                         # maximum text length
    layer_attns: False                                        # activating layer attention
    layer_cross_attns: [False, False, True]                   # activating layer crossattention for each stage
    cond_images_channels: 0                                   # number of conditional input channels
    channels: 1                                               # number of input channels that get trained on 
    channels_out: 1                                           # number of output channels that get trained on 
    cond_on_text: True  # for the style embeds                 # if conditioned on text 
    seg_dec: False
  scheduler:
    schedule: "scaled_linear_beta"
    num_train_timesteps: 1000
    beta_start: 0.0015
    beta_end: 0.0205
    prediction_type: "v_prediction"

semantic_encoder:
  dim: 16
  text_embed_dim: 512
  num_resnet_blocks: 2
  dim_mults: [1,2,4]
  max_text_len: 512
  layer_attns: False 
  layer_cross_attns: [False, False, True]
  cond_images_channels: 0
  channels: 1
  cond_on_text: False  # for the style embeds


stage1:
  base_lr: 0.00005
  disc_lr: 0.0001
  perceptual_weight: 0.002
  adv_weight: 0.005
  kl_weight: 0.00000001
  adv_start: 25
  # ckpt: '/home/peter/PycharmProjects/videoMRI/outputs/aekl_s3_l3_b8.pth'
  params:
    spatial_dims: 3
    in_channels: 1
    out_channels: 1
    num_channels: [32, 64, 128]
#    num_channels: [128, 128]
    latent_channels: 3
    num_res_blocks: 2
    attention_levels: [False, False, False]
#    attention_levels: [False, False]
    with_encoder_nonlocal_attn: False
    with_decoder_nonlocal_attn: False

discriminator:
  params:
    spatial_dims: 3
    num_channels: 96
    num_layers_d: 3
    in_channels: 1

perceptual_network:
  params:
    spatial_dims: 3
    network_type: "squeeze"
    is_fake_3d: True
    fake_3d_ratio: 0.25



cascaded: 
  image_sizes: [64, 128]
  channels: 1
  temporal_downsample_factor: [1, 1]
  text_embed_dim: 512

trainer:
  fp16: False                                                  # if to use automatic mixec precision
  grad_accum_steps: 1.0                                        # number of steps to accumulate the gradients for 
  epoch: 300                                                   # number of epochs 

  gradient_clipping:                                           # where to gradient clip 
    clip_grads: 1.0
  
  timestep_sampler:                                            
    num_sample_steps: 1000                                     # number of timesampling steps for the DM encoding/decoding
    sample: uniform                                            # the way the noise gets sampled 
  
  loss:
    seg: mse                                           # dice, mse
    recon: mse                                          # ssim, mse

  optimizer:
    name: Adam
    params:
        lr: 0.0001
        weight_decay: 0
  
  beta:
    schedule: linear
    linear:
        start: 0.0001
        end: 0.02
    cosine:
        s: 0.008
        max_beta: 0.999

dataset:
  data_path: "/vol/aimspace/projects/ukbb/data/cardiac/cardiac_segmentations/projects/MedMAE"  # datapath   /vol/aimspace/projects/ukbb/data/cardiac/cardiac_segmentations/projects/MedMAE
  # data_path: '/ssd2t/ukbb/MedMAE/original'
  sbj_file: null #"/vol/aimspace/users/bubeckn/MRIDiffusion/twelve-sliced-subjects.txt"         # file which defines the subjects to train on
  deactivate_cache: False
  time_res: 5 # IMPORTANT CHANGE IN 1SCM: 8fps instead of 32fps
  slice_res: 8
  res: 64 # resolution
  fps: 32 # frames per second
  duration: 1 #seconds
  grayscale: False
  use_la: False
  use_seg: False
  cond_slices: [5]
  train_pct: 0.8
  time_idx: null
  mode: fcfs
  normalize: True
  crop_along_bbox: True
  
  train: 
    transforms:
      - name: RandomHorizontalFlip
        params:
            p: 0.0
      - name: RandomVerticalFlip
        params:
            p: 0.0

  test: 
    transforms:
      - name: RandomHorizontalFlip
        params:
            p: 0.0
      - name: RandomVerticalFlip
        params:
            p: 0.0

dataloader: # This is adapted when launching the training script
  batch_size: 8
  num_workers: 8

wandb:
  project: "MRIdiffae"
#  entity: "niklas-bubeck"
  entity: "jzpeterpan"

checkpoint:
  path: "./outputs/diffusion"
  batch_size: 4
  cond_scale: 5.
  log_every_x_it: 1000
  save_every_x_it: 1000

seed: 42
three_d: True