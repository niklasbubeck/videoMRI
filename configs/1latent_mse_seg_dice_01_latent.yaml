unets:
  unet1: # BASE
    dim: 128                                                  # Base dimension of the unet
    text_embed_dim: 512                                       # dimension of the resnet induced embeddings
    num_resnet_blocks: 2                                      # num of resnetblocks to use on each stage
    dim_mults: [1,2,4]                                        # multiplicators of each stage    
    max_text_len: 512                                         # maximum text length
    layer_attns: False                                        # activating layer attention
    layer_cross_attns: [False, False, True]                   # activating layer crossattention for each stage
    cond_images_channels: 0                                   # number of conditional input channels
    channels: 3                                               # number of input channels that get trained on 
    channels_out: 3                                           # number of output channels that get trained on 
    cond_on_text: True  # for the style embeds                 # if conditioned on text 
    seg_dec: True

encoders: 
  encoder1:
    dim: 16
    text_embed_dim: 512
    num_resnet_blocks: 2
    dim_mults: [1,2,4]
    max_text_len: 512
    layer_attns: False 
    layer_cross_attns: [False, False, True]
    cond_images_channels: 0
    channels: 3
    cond_on_text: False  # for the style embeds


cascaded: 
  image_sizes: [64, 128]
  channels: 1
  temporal_downsample_factor: [1, 1]
  text_embed_dim: 512

trainer:
  fp16: False                                                  # if to use automatic mixec precision
  grad_accum_steps: 1.0                                        # number of steps to accumulate the gradients for 
  epoch: 300    
  
  use_latent_space: True    
  aekl_path: "/vol/aimspace/projects/ukbb/data/cardiac/cardiac_segmentations/projects/videoMRI/checkpoint.pth"                                           # number of epochs 

  gradient_clipping:                                           # where to gradient clip 
    clip_grads: 1.0
  
  timestep_sampler:                                            
    num_sample_steps: 1000                                     # number of timesampling steps for the DM encoding/decoding
    sample: uniform                                            # the way the noise gets sampled 
  
  loss:
    seg: mse                                           # dice, mse
    recon: mse 
    tgrad: False
    weight: 0.1                                        # ssim, mse
    tweight: 1

  optimizer:
    name: Adam
    params:
        lr: 0.0001
        weight_decay: 0
  
  beta:
    schedule: linear
    linear:
        start: 0.0001
        end: 0.02
    cosine:
        s: 0.008
        max_beta: 0.999

dataset:
  data_path: "/vol/aimspace/projects/ukbb/data/cardiac/cardiac_segmentations/projects/MedMAE"  # datapath   /vol/aimspace/projects/ukbb/data/cardiac/cardiac_segmentations/projects/MedMAE
  sbj_file: null #"/vol/aimspace/users/bubeckn/MRIDiffusion/twelve-sliced-subjects.txt"         # file which defines the subjects to train on
  deactivate_cache: False
  time_res: 32 # IMPORTANT CHANGE IN 1SCM: 8fps instead of 32fps
  slice_res: 8
  res: 128 # resolution
  fps: 32 # frames per second
  duration: 1 #seconds
  grayscale: False
  use_la: False
  use_seg: False
  cond_slices: [5]
  train_pct: 0.8
  time_idx: null
  mode: random
  normalize: False
  
  train: 
    transforms:
      - name: RandomHorizontalFlip
        params:
            p: 0.0
      - name: RandomVerticalFlip
        params:
            p: 0.0

  test: 
    transforms:
      - name: RandomHorizontalFlip
        params:
            p: 0.0
      - name: RandomVerticalFlip
        params:
            p: 0.0

dataloader: # This is adapted when launching the training script
  batch_size: 8
  num_workers: 8

wandb:
  project: "videoMRI"
  entity: "videomri"

checkpoint:
  path: "./outputs/diffusion"
  batch_size: 4
  cond_scale: 5.
  log_every_x_it: 1000
  save_every_x_it: 1000

seed: 42
three_d: True

general:
  exp_name: aekl_s4_l3_b8
  network: AEKL
  training_epochs: 250
  eval_freq: 10

stage1:
  base_lr: 0.00005
  disc_lr: 0.0001
  perceptual_weight: 0.002
  adv_weight: 0.005
  kl_weight: 0.00000001
  adv_start: 25
  params:
    spatial_dims: 3
    in_channels: 1
    out_channels: 1
    num_channels: [32, 64, 128]
    latent_channels: 3
    num_res_blocks: 2
    attention_levels: [False, False, False]
    with_encoder_nonlocal_attn: False
    with_decoder_nonlocal_attn: False

discriminator:
  params:
    spatial_dims: 3
    num_channels: 96
    num_layers_d: 3
    in_channels: 1

perceptual_network:
  params:
    spatial_dims: 3
    network_type: "squeeze"
    is_fake_3d: True
    fake_3d_ratio: 0.25