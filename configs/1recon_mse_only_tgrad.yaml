unets:
  unet1: # BASE
    dim: 8                                                  # Base dimension of the unet
    text_embed_dim: 512                                       # dimension of the resnet induced embeddings
    num_resnet_blocks: 2                                      # num of resnetblocks to use on each stage
    dim_mults: [1,2,4]                                        # multiplicators of each stage    
    max_text_len: 512                                         # maximum text length
    layer_attns: False                                        # activating layer attention
    layer_cross_attns: [False, False, True]                   # activating layer crossattention for each stage
    cond_images_channels: 0                                   # number of conditional input channels
    channels: 1                                               # number of input channels that get trained on 
    channels_out: 1                                           # number of output channels that get trained on 
    cond_on_text: True  # for the style embeds                 # if conditioned on text 
    seg_dec: False

encoders: 
  encoder1:
    dim: 8
    text_embed_dim: 512
    num_resnet_blocks: 2
    dim_mults: [1,2,4]
    max_text_len: 512
    layer_attns: False 
    layer_cross_attns: [False, False, True]
    cond_images_channels: 0
    channels: 1
    cond_on_text: False  # for the style embeds


cascaded: 
  image_sizes: [64, 128]
  channels: 1
  temporal_downsample_factor: [1, 1]
  text_embed_dim: 512

trainer:
  fp16: False                                                  # if to use automatic mixec precision
  grad_accum_steps: 1.0                                        # number of steps to accumulate the gradients for 
  epoch: 300                                                   # number of epochs 

  gradient_clipping:                                           # where to gradient clip 
    clip_grads: 1.0
  
  timestep_sampler:                                            
    num_sample_steps: 1000                                     # number of timesampling steps for the DM encoding/decoding
    sample: uniform                                            # the way the noise gets sampled 
  
  loss:
    seg: mse                                           # dice, mse
    recon: mse                                          # ssim, mse
    tgrad: True
    weight: 1 
    tweight: 0.1

  optimizer:
    name: Adam
    params:
        lr: 0.0001
        weight_decay: 0
  
  beta:
    schedule: linear
    linear:
        start: 0.0001
        end: 0.02
    cosine:
        s: 0.008
        max_beta: 0.999

dataset:
  data_path: "/vol/aimspace/projects/ukbb/data/cardiac/cardiac_segmentations/projects/MedMAE"  # datapath   /vol/aimspace/projects/ukbb/data/cardiac/cardiac_segmentations/projects/MedMAE
  sbj_file: null #"/vol/aimspace/users/bubeckn/MRIDiffusion/twelve-sliced-subjects.txt"         # file which defines the subjects to train on
  deactivate_cache: False
  time_res: 5 # IMPORTANT CHANGE IN 1SCM: 8fps instead of 32fps
  slice_res: 8
  res: 128 # resolution
  fps: 32 # frames per second
  duration: 1 #seconds
  grayscale: False
  use_la: False
  use_seg: False
  cond_slices: [5]
  train_pct: 0.8
  time_idx: null
  
  train: 
    transforms:
      - name: RandomHorizontalFlip
        params:
            p: 0.0
      - name: RandomVerticalFlip
        params:
            p: 0.0

  test: 
    transforms:
      - name: RandomHorizontalFlip
        params:
            p: 0.0
      - name: RandomVerticalFlip
        params:
            p: 0.0

dataloader: # This is adapted when launching the training script
  batch_size: 8
  num_workers: 8

wandb:
  project: "videoMRI"
  entity: "videomri"

checkpoint:
  path: "./outputs/diffusion"
  batch_size: 4
  cond_scale: 5.
  log_every_x_it: 1000
  save_every_x_it: 1000

seed: 42
three_d: True